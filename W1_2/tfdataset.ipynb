{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Kỹ thuật tensorflow Dataset**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Vai trò của tensorflow Dataset**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vì sao có thể truyền các bộ dữ liệu lớn vào mô hình?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Ta không thể tuyền toàn bộ dữ liệu vào mô hình cùng một lúc vì dữ liệu thường có kích thước lớn hơn RAM máy tính. Các framework deep learning thường hỗ trợ huấn luyện mô hình theo generator. Dữ liệu sẽ được load theo từng batch, tức huấn luyện đến đâu thì khởi tạo đến đó.</p>\n",
    "\n",
    "<p>Tùy theo định dạng dữ liệu là text, image, dataframe, numpy array,... mà chúng ta sẽ sử dụng những module tạo dữ liệu huấn luyện khác nhau.</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Định nghĩa generator**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator có thể coi là một người vay nợ, được quyền sử dụng của người khác nhưng không trả ngay, ta ví dữ liệu như là tiền thì generator sẽ sử dụng dữ liệu vào mục đích của mình. Tuy nhiên dữ liệu sau khi biến đổi không được trả về như các hàm thông thường."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giả sử một người vay $n$ món nợ với cùng lãi suất là 1%/tháng. Để tính lãi suất phải trả của các khoản vay chúng ta có thể sử dụng vòng for và tính để tính kết quả trong 1 lần."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scales of origin balance:  [0.010000000000000009, 0.030301000000000133, 0.061520150601000134, 0.09368527268436089, 0.12682503013196977]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def _interest_rate(month):\n",
    "  return (1+0.01)**month - 1\n",
    "\n",
    "\n",
    "periods = [1, 3, 6, 9, 12]\n",
    "scales = [_interest_rate(month) for month in periods]\n",
    "print('scales of origin balance: ', scales)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuy nhiên nếu sử dụng generator thì chúng ta chỉ việc thay `return` bằng `yield`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scales of origin balance:  [<generator object _gen_interest_rate at 0x0000021F7CFB4DD0>, <generator object _gen_interest_rate at 0x0000021F7CFB4430>, <generator object _gen_interest_rate at 0x0000021F7CFB44A0>, <generator object _gen_interest_rate at 0x0000021F7CFB4660>, <generator object _gen_interest_rate at 0x0000021F7CFB4580>]\n"
     ]
    }
   ],
   "source": [
    "def _gen_interest_rate(month):\n",
    "  yield (1+0.01)**month - 1\n",
    "\n",
    "\n",
    "periods = [1, 3, 6, 9, 12]\n",
    "scales = [_gen_interest_rate(month) for month in periods]\n",
    "print('scales of origin balance: ', scales)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generator sẽ không trả về kết quả ngay mà chỉ tạo sẵn các ô nhớ lưu hàm generator mô tả cách tính lãi suất. Không tốn chi phí thời gian thực hiện phép tính -> đang nợ máy tính kết quả trả về. Chỉ khi nào được chủ nợ gọi tên bằng cách kích hoạt trong ham `next()` thì mới tính kết quả."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.010000000000000009,\n",
       " 0.030301000000000133,\n",
       " 0.061520150601000134,\n",
       " 0.09368527268436089,\n",
       " 0.12682503013196977]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[next(_gen_interest_rate(n)) for n in periods]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lợi thế generator là:\n",
    "- Không sinh toàn bộ dữ liệu cùng một lúc, nâng cao hiệu suất vì sử dụng ít bộ nhớ hơn.\n",
    "- Không phải chờ toàn bộ các vòng lặp được xử lý xong thì mới xử lý tiếp nên tiết kiệm thời gian tính toán "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Cách khởi tạo một Dataset:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset là một class của tensorflow dùng để wrap dữ liệu trước khi truyền vào mô hình để huấn luyện để kiểm soát input X, output y. Ta nên wrap chúng trong `tf.Dataset`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có 2 phương pháp chính để khởi tạo một tf.Dataset trong tensorflow:\n",
    "\n",
    "* **In memory Dataset:** Khởi tạo các dataset ngay từ đầu và dữ liệu được lưu trữ trên memory.\n",
    "* **Generator Dataset:** Dữ liệu được sinh ra theo từng batch và xen kẽ với quá trình huấn luyện từ các hàm khởi tạo generator.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phương pháp `In memory Dataset` sẽ phù hợp với các bộ dữ liệu kích thước nhỏ mà RAM có thể load được. Quá trình huấn luyện theo cách này thì nhanh hơn so với phương pháp `Generator Dataset` vì dữ liệu đã được chuẩn bị sẵn mà không tốn thời gian chờ khởi tạo batch. Tuy nhiên dễ xảy ra `out of memory` trong quá trình huấn luyện.\n",
    "\n",
    "Theo cách `Generator Dataset` chúng ta sẽ qui định cách mà dữ liệu được tạo ra như thế nào thông qua một hàm `generator`. Quá trình huấn luyện đến đâu sẽ tạo batch đến đó. Do đó các bộ dữ liệu big data có thể được load theo từng batch sao cho kích thước vừa được dung lượng RAM. Theo cách huấn luyện này chúng ta có thể huấn luyện được các bộ dữ liệu có kích thước lớn hơn nhiều so với RAM bằng cách chia nhỏ chúng theo batch. Đồng thời có thể áp dụng thêm các step preprocessing data trước khi dữ liệu được đưa vào huấn luyện. Do đó đây thường là phương pháp được ưa chuộng khi huấn luyện các model deep learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 In Memory Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các dữ liệu train và test được load vào bộ nhớ. Tiếp theo chúng ta sẽ khởi tạo Dataset cho những dữ liệu in memory này bằng hàm `tf.data.Dataset.from_tensor_slices()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi đó chúng ta đã có thể fit vào mô hình huấn luyện các dữ liệu được truyền vào `tf.Dataset` là `(X_train, y_train)`.\n",
    "\n",
    "Chúng ta cũng có thể áp dụng các phép biến đổi bằng các hàm như `Dataset.map()` hoặc `Dataset.batch()` để biến đổi dữ liệu trước khi fit vào model. Các bạn xem thêm tại `tf.Dataset`. Chẳng hạn trước khi truyền batch vào huấn luyện tôi sẽ thực hiện chuẩn hóa batch theo phân phối chuẩn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.backend import std, mean\n",
    "from tensorflow.math import reduce_std, reduce_mean\n",
    "\n",
    "def _normalize(X_batch, y_batch):\n",
    "  '''\n",
    "  X_batch: matrix digit images, shape batch_size x 28 x 28\n",
    "  y_batch: labels of digit.\n",
    "  '''\n",
    "  X_batch = tf.cast(X_batch, dtype = tf.float32)\n",
    "  # Padding về 2 chiều các giá trị 0 để được shape là 32 x 32\n",
    "  pad = tf.constant([[0, 0], [2, 2], [2, 2]])\n",
    "  X_batch = tf.pad(X_batch, paddings=pad, mode='CONSTANT', constant_values=0)\n",
    "  X_batch = tf.expand_dims(X_batch, axis=-1)\n",
    "  mean = reduce_mean(X_batch)\n",
    "  std = reduce_std(X_batch)\n",
    "  X_norm = (X_batch-mean)/std\n",
    "  return X_norm, y_batch\n",
    "\n",
    "train_dataset = train_dataset.batch(32).map(_normalize)\n",
    "valid_dataset = valid_dataset.batch(32).map(_normalize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_dataset` và `valid_dataset` lần lượt thực hiện các bước xử lí dữ liệu sau:\n",
    "- Hàm `.batch(32)`: Trích xuất ra từ list `(X_train, y_train)` các batch_size có kích thước là 32.\n",
    "- Hàm `.map(_normalize)`: Mapping đầu vào là các batch `(X_batch, y_batch)` kích thước 32 vào hàm số `_normalize()`. Kết quả trả về là giá trị đã chuẩn hóa theo batch của `x_batch` và `y_batch`. Dữ liệu này sẽ được sử dụng để huấn luyện model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenet_1.00_32 (Function  (None, 1, 1, 1024)       3228288   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                10250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,238,538\n",
      "Trainable params: 3,216,650\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "base_extractor = MobileNet(input_shape = (32, 32, 1), include_top = False, weights = None)\n",
    "flat = Flatten()\n",
    "den = Dense(10, activation='softmax')\n",
    "model = Sequential([base_extractor, \n",
    "                   flat,\n",
    "                   den])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 485s 257ms/step - loss: 0.4860 - accuracy: 0.8490 - val_loss: 0.1739 - val_accuracy: 0.9488\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 463s 247ms/step - loss: 0.1550 - accuracy: 0.9571 - val_loss: 0.1340 - val_accuracy: 0.9625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21f101e3160>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(Adam(), loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(train_dataset, validation_data= valid_dataset,epochs = 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Generator Dataset**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khởi tạo từ Generator chúng ta sẽ không phải ghi nhớ toàn bộ dữ liệu vào RAM. Tạo dữ liệu trong quá trình huấn luyện ở mỗi lượt fit từng batch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ khởi tạo data generator để sinh dữ liệu cho mô hình phân loại món ăn theo địa phương."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>food</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hochiminh</td>\n",
       "      <td>hủ tiếu bò sài gòn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hanoi</td>\n",
       "      <td>cháo lòng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hochiminh</td>\n",
       "      <td>hủ tiếu nam vang sài gòn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hochiminh</td>\n",
       "      <td>bánh phở</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hochiminh</td>\n",
       "      <td>hủ tiếu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         city                      food\n",
       "9   hochiminh        hủ tiếu bò sài gòn\n",
       "6       hanoi                 cháo lòng\n",
       "8   hochiminh  hủ tiếu nam vang sài gòn\n",
       "11  hochiminh                  bánh phở\n",
       "12  hochiminh                   hủ tiếu"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hanoi = ['bún chả hà nội', 'chả cá lã vọng hà nội', 'cháo lòng hà nội', 'ô mai sấu hà nội', 'ô mai', 'chả cá', 'cháo lòng']\n",
    "hochiminh = ['bánh canh sài gòn', 'hủ tiếu nam vang sài gòn', 'hủ tiếu bò sài gòn', 'banh phở sài gòn', 'bánh phở', 'hủ tiếu']\n",
    "city = ['hanoi'] * len(hanoi) + ['hochiminh'] * len(hochiminh)\n",
    "corpus = hanoi+hochiminh\n",
    "\n",
    "data = pd.DataFrame({'city': city, 'food': corpus})\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voc(object):\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        self.dictionary = {'unk': 0}\n",
    "        self._initialize_dict(corpus)\n",
    "\n",
    "    def _add_dict_sentence(self, sentence):\n",
    "        words = sentence.split(' ')\n",
    "        for word in words:\n",
    "            if word not in self.dictionary.keys():\n",
    "                max_indice = max(self.dictionary.values())\n",
    "                self.dictionary[word] = (max_indice + 1)\n",
    "\n",
    "    def _initialize_dict(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            self._add_dict_sentence(sentence)\n",
    "\n",
    "    def _tokenize(self, sentence):\n",
    "        words = sentence.split(' ')\n",
    "        token_seq = [self.dictionary[word] for word in words]\n",
    "        return np.array(token_seq)\n",
    "\n",
    "voc = Voc(corpus = corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk': 0,\n",
       " 'bún': 1,\n",
       " 'chả': 2,\n",
       " 'hà': 3,\n",
       " 'nội': 4,\n",
       " 'cá': 5,\n",
       " 'lã': 6,\n",
       " 'vọng': 7,\n",
       " 'cháo': 8,\n",
       " 'lòng': 9,\n",
       " 'ô': 10,\n",
       " 'mai': 11,\n",
       " 'sấu': 12,\n",
       " 'bánh': 13,\n",
       " 'canh': 14,\n",
       " 'sài': 15,\n",
       " 'gòn': 16,\n",
       " 'hủ': 17,\n",
       " 'tiếu': 18,\n",
       " 'nam': 19,\n",
       " 'vang': 20,\n",
       " 'bò': 21,\n",
       " 'banh': 22,\n",
       " 'phở': 23}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta sẽ khởi tạo một `random_generator có tác dụng lựa chọn ngẫu nhiên một tên món ăn trong corpus và tokenize chúng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.float16, name=None), TensorSpec(shape=(), dtype=tf.float16, name=None))>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "cat_indices = {\n",
    "    'hanoi': 0,\n",
    "    'hochiminh': 1\n",
    "}\n",
    "\n",
    "def generators():\n",
    "  i = 0\n",
    "  while True:\n",
    "    i = np.random.choice(data.shape[0])\n",
    "    sentence = data.iloc[i, 1]\n",
    "    x_indice = voc._tokenize(sentence)\n",
    "    label = data.iloc[i, 0]\n",
    "    y_indice = cat_indices[label]\n",
    "    yield x_indice, y_indice\n",
    "    i += 1\n",
    "\n",
    "random_generator = tf.data.Dataset.from_generator(\n",
    "    generators,\n",
    "    output_types = (tf.float16, tf.float16),\n",
    "    output_shapes = ((None,), ())\n",
    ")\n",
    "\n",
    "random_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[17. 18.  0.  0.  0.  0.]\n",
      " [10. 11. 12.  3.  4.  0.]\n",
      " [13. 23.  0.  0.  0.  0.]\n",
      " [17. 18.  0.  0.  0.  0.]\n",
      " [17. 18. 19. 20. 15. 16.]\n",
      " [13. 14. 15. 16.  0.  0.]\n",
      " [ 8.  9.  0.  0.  0.  0.]\n",
      " [ 1.  2.  3.  4.  0.  0.]\n",
      " [10. 11. 12.  3.  4.  0.]\n",
      " [17. 18.  0.  0.  0.  0.]\n",
      " [ 8.  9.  0.  0.  0.  0.]\n",
      " [17. 18. 19. 20. 15. 16.]\n",
      " [ 2.  5.  6.  7.  3.  4.]\n",
      " [13. 14. 15. 16.  0.  0.]\n",
      " [17. 18.  0.  0.  0.  0.]\n",
      " [17. 18.  0.  0.  0.  0.]\n",
      " [17. 18.  0.  0.  0.  0.]\n",
      " [ 8.  9.  3.  4.  0.  0.]\n",
      " [ 1.  2.  3.  4.  0.  0.]\n",
      " [ 8.  9.  0.  0.  0.  0.]], shape=(20, 6), dtype=float16)\n",
      "tf.Tensor([1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0.], shape=(20,), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "random_generator_batch = random_generator.shuffle(20).padded_batch(20, padded_shapes=([None], []))\n",
    "sequence_batch, label = next(iter(random_generator_batch))\n",
    "\n",
    "print(sequence_batch)\n",
    "print(label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hàm `shuffle(20)` trộn lẫn ngẫu nhiên dữ liệu. Sau đó dữ liệu được chia thành những batch có kích thước là 20.\n",
    "- Hàm `padded_batch()` padding giá trị 0 sao cho bằng với độ dài của câu dài nhất."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.1 Sử dụng ImageGenerator**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageGenerator cũng là một dạng data generator được xây dựng trên framework keras và dành riêng cho dữ liệu ảnh.\n",
    "\n",
    "Đây là một high level function nên cú pháp đơn giản, rất dễ sử dụng nhưng khả năng tùy biến và can thiệp sâu vào dữ liệu kém.\n",
    "\n",
    "Khi khởi tạo ImageGenerator chúng ta sẽ khai báo các thủ tục preprocessing image trước khi đưa vào huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truyền dữ liệu vào mô hình bằng hàm `flow_from_directory()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.2 Customize ImageGenerator**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu sử dụng các hàm mặc định của image preprocessing trong ImageGenerator thì sẽ giới hạn bởi một số phép biến đổi mà hàm này hỗ trợ, khó can thiệp sâu nên phải Customize lại ImageGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Dog-Cat-Classifier'...\n",
      "Updating files:  65% (1087/1672)\n",
      "Updating files:  66% (1104/1672)\n",
      "Updating files:  67% (1121/1672)\n",
      "Updating files:  68% (1137/1672)\n",
      "Updating files:  69% (1154/1672)\n",
      "Updating files:  70% (1171/1672)\n",
      "Updating files:  71% (1188/1672)\n",
      "Updating files:  72% (1204/1672)\n",
      "Updating files:  73% (1221/1672)\n",
      "Updating files:  74% (1238/1672)\n",
      "Updating files:  75% (1254/1672)\n",
      "Updating files:  76% (1271/1672)\n",
      "Updating files:  77% (1288/1672)\n",
      "Updating files:  78% (1305/1672)\n",
      "Updating files:  79% (1321/1672)\n",
      "Updating files:  80% (1338/1672)\n",
      "Updating files:  81% (1355/1672)\n",
      "Updating files:  82% (1372/1672)\n",
      "Updating files:  83% (1388/1672)\n",
      "Updating files:  84% (1405/1672)\n",
      "Updating files:  85% (1422/1672)\n",
      "Updating files:  86% (1438/1672)\n",
      "Updating files:  87% (1455/1672)\n",
      "Updating files:  88% (1472/1672)\n",
      "Updating files:  89% (1489/1672)\n",
      "Updating files:  90% (1505/1672)\n",
      "Updating files:  91% (1522/1672)\n",
      "Updating files:  92% (1539/1672)\n",
      "Updating files:  93% (1555/1672)\n",
      "Updating files:  94% (1572/1672)\n",
      "Updating files:  95% (1589/1672)\n",
      "Updating files:  96% (1606/1672)\n",
      "Updating files:  97% (1622/1672)\n",
      "Updating files:  98% (1639/1672)\n",
      "Updating files:  99% (1656/1672)\n",
      "Updating files:  99% (1661/1672)\n",
      "Updating files: 100% (1672/1672)\n",
      "Updating files: 100% (1672/1672), done.\n",
      "Filtering content: 100% (2/2)\n",
      "Filtering content: 100% (2/2), 131.27 MiB | 507.00 KiB/s\n",
      "Filtering content: 100% (2/2), 131.27 MiB | 503.00 KiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ardamavi/Dog-Cat-Classifier.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is New Volume\n",
      " Volume Serial Number is 003F-6D10\n",
      "\n",
      " Directory of c:\\Users\\Chi Khang\\Documents\\Internship\\W1\n",
      "\n",
      "01/28/2023  09:42 AM    <DIR>          .\n",
      "01/28/2023  09:42 AM    <DIR>          ..\n",
      "01/28/2023  09:44 AM    <DIR>          Dog-Cat-Classifier\n",
      "01/28/2023  10:10 AM            27,406 tfdataset.ipynb\n",
      "01/15/2023  08:03 PM         1,189,164 Xception.ipynb\n",
      "               2 File(s)      1,216,570 bytes\n",
      "               3 Dir(s)  74,749,116,416 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "import cv2\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,\n",
    "                 all_filenames, \n",
    "                 labels, \n",
    "                 batch_size, \n",
    "                 index2class,\n",
    "                 input_dim,\n",
    "                 n_channels,\n",
    "                 n_classes=2, \n",
    "                 shuffle=True):\n",
    "        '''\n",
    "        all_filenames: list toàn bộ các filename\n",
    "        labels: nhãn của toàn bộ các file\n",
    "        batch_size: kích thước của 1 batch\n",
    "        index2class: index của các class\n",
    "        input_dim: (width, height) đầu vào của ảnh\n",
    "        n_channels: số lượng channels của ảnh\n",
    "        n_classes: số lượng các class \n",
    "        shuffle: có shuffle dữ liệu sau mỗi epoch hay không?\n",
    "        '''\n",
    "        self.all_filenames = all_filenames\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.index2class = index2class\n",
    "        self.input_dim = input_dim\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        return:\n",
    "          Trả về số lượng batch/1 epoch\n",
    "        '''\n",
    "        return int(np.floor(len(self.all_filenames) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        params:\n",
    "          index: index của batch\n",
    "        return:\n",
    "          X, y cho batch thứ index\n",
    "        '''\n",
    "        # Lấy ra indexes của batch thứ index\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # List all_filenames trong một batch\n",
    "        all_filenames_temp = [self.all_filenames[k] for k in indexes]\n",
    "\n",
    "        # Khởi tạo data\n",
    "        X, y = self.__data_generation(all_filenames_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        '''\n",
    "        Shuffle dữ liệu khi epochs end hoặc start.\n",
    "        '''\n",
    "        self.indexes = np.arange(len(self.all_filenames))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, all_filenames_temp):\n",
    "        '''\n",
    "        params:\n",
    "          all_filenames_temp: list các filenames trong 1 batch\n",
    "        return:\n",
    "          Trả về giá trị cho một batch.\n",
    "        '''\n",
    "        X = np.empty((self.batch_size, *self.input_dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Khởi tạo dữ liệu\n",
    "        for i, fn in enumerate(all_filenames_temp):\n",
    "            # Đọc file từ folder name\n",
    "            img = cv2.imread(fn)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, self.input_dim)\n",
    "            label = fn.split('/')[3]\n",
    "            label = self.index2class[label]\n",
    "    \n",
    "            X[i,] = img\n",
    "\n",
    "            # Lưu class\n",
    "            y[i] = label\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import glob2\n",
    "\n",
    "dict_labels = {\n",
    "    'dog': 0,\n",
    "    'cat': 1\n",
    "}\n",
    "\n",
    "root_folder = 'Dog-Cat-Classifier\\\\Data\\\\Train_Data'\n",
    "fns = glob2.glob(root_folder)\n",
    "print(len(fns))\n",
    "\n",
    "image_generator = DataGenerator(\n",
    "    all_filenames = fns,\n",
    "    labels = None,\n",
    "    batch_size = 32,\n",
    "    index2class = dict_labels,\n",
    "    input_dim = (224, 224),\n",
    "    n_channels = 3,\n",
    "    n_classes = 2,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "X, y = image_generator.__getitem__(1)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X, y \u001b[39m=\u001b[39m image_generator\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(\u001b[39m0\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(X)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(y)\n",
      "Cell \u001b[1;32mIn [28], line 57\u001b[0m, in \u001b[0;36mDataGenerator.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     54\u001b[0m all_filenames_temp \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_filenames[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m indexes]\n\u001b[0;32m     56\u001b[0m \u001b[39m# Khởi tạo data\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__data_generation(all_filenames_temp)\n\u001b[0;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "Cell \u001b[1;32mIn [28], line 85\u001b[0m, in \u001b[0;36mDataGenerator.__data_generation\u001b[1;34m(self, all_filenames_temp)\u001b[0m\n\u001b[0;32m     83\u001b[0m img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(img, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     84\u001b[0m img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(img, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_dim)\n\u001b[1;32m---> 85\u001b[0m label \u001b[39m=\u001b[39m fn\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m3\u001b[39;49m]\n\u001b[0;32m     86\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex2class[label]\n\u001b[0;32m     88\u001b[0m X[i,] \u001b[39m=\u001b[39m img\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chikhang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55b8e25920f24e5fb540a5617ed9e83a094b314ca164b2901879d0fca1c9fbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
